#12월 29일 생성

#데이터가 충분하다면 데이터를 더 생성할 필요는 없다. 데이터를 억지로 늘려도
#데이터의 가치는 늘어나기 전과 거의 차이가 없다. 그럼에도 불균형한 데이터를
#조정할 때에는 데이터를 생성해야 한다.

#머신러닝에서 모델을 만들 때 학습 데이터가 불균형하면 예측 정확도가 떨어지는 경우
#가 많다. 데이터 불균형이란 장애를 예측하는 모델을 학습 데이터로 만들 때
#정상 데이터는 100만건이지만 장애 데이터는 100건 정도밖에 없는 불균형한 상태를
#의미한다. 즉 어느 분류에 속하는 데이터 수가 다른 분류에 속하는 데이터에 비해
#매우 적은 상태이다. 

#불균형한 데이터를 대처하는 방법은 두 가지가 있다. 첫 번째는 머신러닝 모델
#을 만들 때 가중치를 부여하는 것이다. 모든 머신러닝 모델에 적용할 수 있는
#것은 아니지만 원본 데이터를 변경하지 않고 적은 데이터에 가중치를 올려
#불균형한 데이터로 학습시킬 수 있다. 두번째는 데이터를 변경하여 불균형한
#상태를 해소하는 방법이다.
#이 방법은 크게 세 종류로 나눌 수 있다.
# 적은 데이터를 늘리는 오버샘플링
# 많은 데이터를 줄이는 언더샘플링
# 두 가지 방법을 모두 사용하는 방법

#여기서는 오버샘플링과 언더샘플링의 대표적인 기법을 소개한다. 머신러닝 모델을
#만들 때 가중치를 부여하여 불균형을 해소하는 방법은 이 책에서는 자세히 다루지
#않는다. 가중치를 부여하는 방법은 머신러닝 모델의 종류에 따라 특징이 달라지고
#사용하기 까다로우며 제공되는 라이브러리에 따라서는 이용할 수 없는 경우도 있다.
#먼저 데이터 조작으로 불균형 데이터를 다루는 방법을 익히길 바란다.

#데이터 분할과 마찬가지로 머신러닝과 관련된 라이브러리가 풍부한 파이선이나
#R은 샘플링을 구현하기 위한 기능이 제공된다.

#6.1 언더샘플링으로 데이터 불균형 조정하기
#언더샘플링은 오버샘플링과 비교해 상대적으로 쉬운 작업이다. 왜냐하면 
#언더샘플링은 데이터를 줄이는 작업이기 때문에 데이터를 선택하는 방법만
#고려하면 되고, 데이터를 생성할 필요는 없기 때문이다. 데이터 선택 방법은
#중복선택(같은 데이터를 두번 이상 선택)을 허용하지 않는 것이 좋다. 
#중복선택으로 특정 데이터가 여러 번 선택되면 데이터가 편중될 수 있어
#과학습이 발생하기 쉽다.

#언더샘플링을 할 때의 랜덤 샘플링은 4장이나 5장에서 사용한 방법으로 구현할 수
#있다. 샘플링 수가 적을 떄 데이터 편향이 없는 랜덤 샘플링을 구현하기 위해
#미리 데이터를 클러스터링하여 작성된 클러스터별로 샘플링을 실행하는 방법이 있다
#머신러닝에서 클러스터링과 언더샘플링을 동시에 실행하는 경우도 있지만
#흔한 경우는 아니기 때문에 다루지 않는다. 그러나 언더샘플링은 데이터를 
#걸러서 정보량을 적게 만드는 방법이기 때문에 되도록 사용하지 않는 편이 좋다.

#메모리나 계산량을 줄이기 위해 어쩔 수 없이 사용하는 경우도 있겠지만 불균형한 
#데이터를 조정하려면 되도록 오버샘플링을 이용하자 다만 적은 데이터에서 대량의
#데이터를 오버샘플링으로 생성하면 앞서 언급한 데이터가 편중된다. 이때는
#오버샘플링과 언더샘플링을 같이 사용하자. 즉 오버샘플링으로 악역향이 미치지
#않는 정도 까지만 데이터 양이 적은 쪽 데이터를 늘리고 언더샘플링으로
#불균형이 해소되는 정도까지만 데이터양이 많은 쪽 데이터를 줄이는 것이 효과적이다.

#언더샘플링 코드는 4장이나 5장에서 설명한 방법과 같다. 다른점이 있다면
#데이터 불균형을 해소하기 위해 어느 정도 샘플링을 하면 좋은가를
#미리 계산한다는 것이다. 따라서 언더샘플링 해설은 생략한다.